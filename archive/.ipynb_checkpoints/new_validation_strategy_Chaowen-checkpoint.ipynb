{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"6\"><b>Showcasing new validation strategy</b></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unchanged codes\n",
    "Unchanged codes are lumped together below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble, add some code for formatting, not related tto validation strategy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "plt.rcParams['savefig.dpi'] = 75\n",
    "\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.serif'] = \"cm\"\n",
    "plt.rcParams['text.latex.preamble']= r\"\\usepackage{subdepth}, \\usepackage{type1cm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of MOFs shared by two datasets are: 89.\n"
     ]
    }
   ],
   "source": [
    "# read the 36-descriptor data\n",
    "df36Descriptor = pd.read_excel('data/ML_data/descriptor_used.xlsx',header=4,index_col=1)\n",
    "\n",
    "# clean up the column\n",
    "columns = [df36Descriptor.columns[1]] + df36Descriptor.columns[3: -11].tolist()\n",
    "\n",
    "newColumns = {}\n",
    "for ci in columns:\n",
    "    if ' ' in ci:\n",
    "        newColumns[ci] = ci.split(' ',1)[0]\n",
    "    elif '(' in ci:\n",
    "        newColumns[ci] = ci.split('(',1)[0]\n",
    "    else:\n",
    "        newColumns[ci] = ci\n",
    "\n",
    "dfShortNames = df36Descriptor[columns].rename(columns=newColumns)\n",
    "\n",
    "# reduce columns to only contain MOF features\n",
    "shared_descriptor = [col for col in dfShortNames.columns if col in newColumns]\n",
    "dfMLReduced = dfShortNames[shared_descriptor]\n",
    "\n",
    "# the MOFs in \"dfMLReduced\" and adsorption data sets are different, so it is necessary to match the MOFs in two datasets\n",
    "def datasetMatch(MOFName):\n",
    "    dfML= dfMLReduced[dfMLReduced['MOF'].isin(MOFName)].drop_duplicates()\n",
    "    matchedMOFIndex=np.isin(MOFName, dfML['MOF'].values)\n",
    "    return matchedMOFIndex, dfML\n",
    "\n",
    "# read flexibility data\n",
    "flexibilityList=os.listdir('data/flexibility_data/y_data/adsorption_data') # obtain list of csv files for 9 adsorption uptakes\n",
    "flexivilityData=[]\n",
    "adsorbateNameList = []\n",
    "\n",
    "for i, name in enumerate(flexibilityList):\n",
    "    # read csv files for certain adsorption uptakes\n",
    "    df = pd.read_csv('data/flexibility_data/y_data/adsorption_data/' + name)\n",
    "    \n",
    "    # obtain the rigid value\n",
    "    rigidValue = np.array(df[df.columns[1]], dtype = float)\n",
    "    \n",
    "    # obtain the flexible mean value\n",
    "    flexValue = np.mean(np.array(df[df.columns[2:]],dtype=float),axis=1)\n",
    "    \n",
    "    # obtain the adsorbate label\n",
    "    label = np.array([name.split(\"_\")[1] for x in range(0,len(flexValue))],dtype=str)\n",
    "    adsorbateNameList.append(name.split(\"_\")[1])\n",
    "    \n",
    "    # stack the rigid value, flexible mean value and the adsorbate label\n",
    "    singleSet = np.column_stack([rigidValue,flexValue,label])\n",
    "\n",
    "    if i == 0:\n",
    "        # obtain the name list of MOFs\n",
    "        MOFNameTemp = np.array(df[df.columns[0]], dtype = str)\n",
    "        MOFName = [x.split(\"_\")[0] for x in MOFNameTemp]\n",
    "        \n",
    "        # search the MOF name in \"dfMLReduced\", generating dfML\n",
    "        matchedMOFIndex, dfML = datasetMatch(MOFName)\n",
    "        print(\"The number of MOFs shared by two datasets are: {:d}.\".format(dfML.shape[0]))\n",
    "        \n",
    "        # generating flexibilityData as \"y\"\n",
    "        flexibilityData = singleSet[matchedMOFIndex,:].copy()\n",
    "    else:\n",
    "        # concatenate \"y\"\n",
    "        flexibilityData = np.concatenate([flexibilityData.copy(),singleSet[matchedMOFIndex,:].copy()])\n",
    "\n",
    "# manually add adsorbate descriptors\n",
    "\n",
    "# Mw/gr.mol-1, Tc/K, Pc/bar, ω, Tb/K, Tf/K\n",
    "\n",
    "adsorbateData=np.array([\n",
    "    ['xenon',131.293,289.7,58.4,0.008,164.87,161.2], \n",
    "    ['butane',58.1,449.8,39.5,0.3,280.1,146.7], \n",
    "    ['propene',42.1,436.9,51.7,0.2,254.8,150.6], \n",
    "    ['ethane',30.1,381.8,50.3,0.2,184.0,126.2], \n",
    "    ['propane',44.1,416.5,44.6,0.2,230.1,136.5], \n",
    "    ['CO2',44.0,295.9,71.8,0.2,317.4,204.9], \n",
    "    ['ethene',28.054,282.5,51.2,0.089,169.3,228], \n",
    "    ['methane',16.04,190.4,46.0,0.011,111.5,91],\n",
    "    ['krypton',83.798,209.4,55.0,0.005,119.6,115.6]])\n",
    "\n",
    "adsorbateData.shape\n",
    "adDf = pd.DataFrame(data=adsorbateData, columns=[\"adsorbate\", \"Mw/gr.mol-1\", \"Tc/K\", \"Pc/bar\", \"ω\", \"Tb/K\", \"Tf/K\"])\n",
    "\n",
    "# sort the dataframe based on adsorbateNameList\n",
    "sorterIndex = dict(zip(adsorbateNameList,range(len(adsorbateNameList))))\n",
    "adDf['an_Rank'] = adDf['adsorbate'].map(sorterIndex)\n",
    "adDf.sort_values(['an_Rank'],ascending = [True], inplace = True)\n",
    "adDf.drop('an_Rank', 1, inplace = True)\n",
    "adDfFloat = adDf.iloc[:, 1:].astype(np.float)\n",
    "adDfFloat[\"adsorbate\"] = adDf[\"adsorbate\"]\n",
    "\n",
    "\n",
    "# replicate dfML for 9 adsorbates\n",
    "dfMLReplicate = pd.concat([dfML]*9)\n",
    "\n",
    "# replicate adDf for 89 MOFs\n",
    "adDfReplicate = pd.DataFrame(np.repeat(adDfFloat.values,89,axis=0))\n",
    "adDfReplicate.columns = adDfFloat.columns\n",
    "\n",
    "# concatenate two datasets\n",
    "dfMLReplicate.reset_index(drop=True, inplace=True)\n",
    "adDfReplicate.reset_index(drop=True, inplace=True)\n",
    "XAllDescriptor = pd.concat([dfMLReplicate, adDfReplicate],axis=1)\n",
    "\n",
    "X = np.concatenate((XAllDescriptor.iloc[:, 1:-1], flexibilityData[:, 0].reshape(-1, 1)),axis=1).astype(np.float)\n",
    "y = flexibilityData[:, 1].astype('float64') .reshape(-1,1)\n",
    "\n",
    "\n",
    "# feature scaling\n",
    "X_scaled = (X - X.mean(axis=0))/X.std(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data set split\n",
    "## Validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# combine the unscaled and scaled X, so that they can be split together\n",
    "X_combined = np.concatenate((X, X_scaled), axis=1)\n",
    "\n",
    "# ---------------------------- don't touch the validation set ----------------------------\n",
    "X_train_test_combined, X_validation_combined, y_train_test, y_validation = train_test_split(X_combined, \\\n",
    "                                                                                            y, test_size=0.25)\n",
    "X_validation, X_validation_scaled = X_validation_combined[:, :35], X_validation_combined[:, 35:]\n",
    "# ---------------------------- don't touch the validation set ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold on train-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_train_test_combined)):\n",
    "    \n",
    "    # initialize sets\n",
    "    if i == 0:\n",
    "        X_train_combined_5fold = np.zeros(X_train_test_combined[train_index].shape + (5,))\n",
    "        X_test_combined_5fold = np.zeros(X_train_test_combined[test_index].shape + (5,))\n",
    "        y_train_5fold = np.zeros(y_train_test[train_index].shape + (5,))\n",
    "        y_test_5fold = np.zeros(y_train_test[test_index].shape + (5,))\n",
    "        \n",
    "    X_train_combined_5fold[:, :, i], X_test_combined_5fold[:, :, i] = X_train_test_combined[train_index], X_train_test_combined[test_index]\n",
    "    y_train_5fold[:, :, i], y_test_5fold[:, :, i] = y_train_test[train_index], y_train_test[test_index]\n",
    "\n",
    "X_train_5fold, X_train_scaled_5fold = X_train_combined_5fold[:, :35], X_train_combined_5fold[:, 35:]\n",
    "X_test_5fold, X_test_scaled_5fold = X_test_combined_5fold[:, :35], X_test_combined_5fold[:, 35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show how to use 5-fold sets\n",
    "\n",
    "The code below is only meant for showing how to use 5-fold strategy for training models.\n",
    "<span style=\"color:red\">They are **not** examples of the whole modeling work</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without hyperparameter tuning - using multi-linear regression as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\tr2=0.987\t<-best performance\n",
      "Fold 1:\tr2=0.980\n",
      "Fold 2:\tr2=0.979\n",
      "Fold 3:\tr2=0.981\n",
      "Fold 4:\tr2=0.962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LR_r2_list = []\n",
    "LR_coef_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    LR_model = LinearRegression()\n",
    "    LR_model.fit(X_train_5fold[:, :, i], y_train_5fold[:, :, i])\n",
    "    \n",
    "    LR_r2_list.append(LR_model.score(X_test_5fold[:, :, i], y_test_5fold[:, :, i]))\n",
    "    LR_coef_list.append(LR_model.coef_)\n",
    "\n",
    "\n",
    "best_i = np.array(LR_r2_list).argmax()\n",
    "LR_coef_best = LR_coef_list[best_i]\n",
    "\n",
    "for i in range(5):\n",
    "    if i != best_i:\n",
    "        print(\"Fold {}:\\tr2={:.3f}\".format(i, LR_r2_list[i]))\n",
    "    else:\n",
    "        print(\"Fold {}:\\tr2={:.3f}\\t<-best performance\".format(i, LR_r2_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With hyperparameter tuning - using rbf kernel regression as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\tr2=0.016,\toptimal gamma=2e+02\n",
      "Fold 1:\tr2=0.075,\toptimal gamma=5e-01\t\t<-best performance\n",
      "Fold 2:\tr2=0.003,\toptimal gamma=5e+03\n",
      "Fold 3:\tr2=0.023,\toptimal gamma=5e+01\n",
      "Fold 4:\tr2=0.022,\toptimal gamma=2e+02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "sigmas = np.array([1E-4, 5E-4, 1E-3, 5E-3, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 15, 20, 25, 30, 40, 50, 60])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "r2_matrix = np.zeros((gammas.size, 3))\n",
    "\n",
    "rbf_r2_list = []\n",
    "rbf_coef_list = []\n",
    "rbf_gamma_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    rbf_coef_temp_list = [] # temporarily store parameter values for each gamma\n",
    "    \n",
    "    for j, gamma in enumerate(gammas):\n",
    "    \n",
    "        X_train_kernel = rbf_kernel(X_train_5fold[:, :, i], X_train_5fold[:, :, i], gamma = gamma)\n",
    "        X_test_kernel = rbf_kernel(X_test_5fold[:, :, i], X_train_5fold[:, :, i], gamma = gamma)\n",
    "        model_rbf = LinearRegression()\n",
    "        model_rbf.fit(X_train_kernel, y_train_5fold[:, :, i])\n",
    "\n",
    "        r2_matrix[j, 0] = gamma\n",
    "        r2_matrix[j, 1] = model_rbf.score(X_train_kernel, y_train_5fold[:, :, i])\n",
    "        r2_matrix[j, -1] = model_rbf.score(X_test_kernel, y_test_5fold[:, :, i])\n",
    "        \n",
    "        rbf_coef_temp_list.append(model_rbf.coef_)\n",
    "\n",
    "    # For each fold, record the optimal gamma and the corresponding parameter values and r2\n",
    "    n = r2_matrix[:, -1].argmax()\n",
    "    rbf_r2_list.append(r2_matrix[n, -1])\n",
    "    rbf_gamma_list.append(r2_matrix[n, 0])\n",
    "    rbf_coef_list.append(rbf_coef_temp_list[n])\n",
    "\n",
    "\n",
    "best_i = np.array(rbf_r2_list).argmax()\n",
    "rbf_gamma_best = rbf_gamma_list[best_i]\n",
    "rbf_coef_best = rbf_coef_list[best_i]\n",
    "\n",
    "for i in range(5):\n",
    "    if i != best_i:\n",
    "        print(\"Fold {}:\\tr2={:.3f},\\toptimal gamma={:.0e}\".format(i, rbf_r2_list[i], rbf_gamma_list[i]))\n",
    "    else:\n",
    "        print(\"Fold {}:\\tr2={:.3f},\\toptimal gamma={:.0e}\\t\\t<-best performance\".format(i, rbf_r2_list[i], rbf_gamma_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Again, the code above does not correspond to the entire work for a model. More work can be done, e.g., \n",
    "    - detailed analysis of results\n",
    "    - more sophisticated hyperparameter tuning\n",
    "    - comparison of using unscaled and scaled data\n",
    "    - comparison with other models, especially for pure rbf, KRR and LASSO\n",
    "- To use the scaled X, replace `X_train_5fold`, `X_test_5fold` with `X_train_scaled_5fold`, `X_test_scaled_5fold`\n",
    "- Remember to record the values of the parameters and the hyperparameters of the best model, as they will be used for future comparison. For example, in \"rbf kernel regression\" above, I have recorded the optimal hyperparameter in `rbf_gamma_best` and the corresponding optimal parameters in `rbf_coef_best`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO Regression - Multi-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 35)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_5fold[:, :, 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-633cf979bf45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mLASSO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mLASSO_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLASSO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mLASSO_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_5fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_5fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                      \u001b[0;31m#best_alpha = LASSO_search.best_estimator_.alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     r_2 = LASSO_search.best_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         scorers, self.multimetric_ = _check_multimetric_scoring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mcheck_cv\u001b[0;34m(cv, y, classifier)\u001b[0m\n\u001b[1;32m   2059\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'split'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNSPLIT_WARNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34m\"k-fold cross-validation requires at least one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0;34m\" train/test split by setting n_splits=2 or more,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \" got n_splits={0}.\".format(n_splits))\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])\n",
    "\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "lasso_alpha_list = []\n",
    "lasso_r2_list = []\n",
    "lasso_coef_list = []\n",
    "lasso_num_dropped = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    LASSO = Lasso()\n",
    "    LASSO_search = GridSearchCV(LASSO, parameter_set, cv=1)\n",
    "    LASSO_search.fit(X_train_5fold[:, :, i], y_train_5fold[:, :, i])\n",
    "                     #best_alpha = LASSO_search.best_estimator_.alpha\n",
    "#     r_2 = LASSO_search.best_score_\n",
    "#     coeffs = LASSO_search.best_estimator_.coef_\n",
    "#     nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "#     num_dropped = len(nonzero))\n",
    "    \n",
    "#     lasso_alpha_list.append(best_alpha)\n",
    "#     lasso_r2_list.append(r_2)\n",
    "#     lasso_coef_list.append(coeffs)\n",
    "#     lasso_num_dropped.append(num_dropped)\n",
    "\n",
    "# #finer GridSearchCV\n",
    "# alphas = np.array([1e-5, 5e-5, 8e-5, 1e-4, 2e-4, 3e-4, 4e-4, 5e-4, 1e-3])\n",
    "\n",
    "# parameter_set = {'alpha':alphas}\n",
    "\n",
    "# LASSO2 = Lasso()\n",
    "# LASSO_search2 = GridSearchCV(LASSO, parameter_set, cv =5)\n",
    "# LASSO_search2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO Regression with RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "MarkdownCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "361px",
    "width": "435px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
