{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"6\"><b>Showcasing new validation strategy with GridSearchCV</b></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unchanged codes\n",
    "Unchanged codes are lumped together below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble, add some code for formatting, not related tto validation strategy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "plt.rcParams['savefig.dpi'] = 75\n",
    "\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.serif'] = \"cm\"\n",
    "plt.rcParams['text.latex.preamble']= r\"\\usepackage{subdepth}, \\usepackage{type1cm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of MOFs shared by two datasets are: 89.\n"
     ]
    }
   ],
   "source": [
    "# read the 36-descriptor data\n",
    "df36Descriptor = pd.read_excel('data/ML_data/descriptor_used.xlsx',header=4,index_col=1)\n",
    "\n",
    "# clean up the column\n",
    "columns = [df36Descriptor.columns[1]] + df36Descriptor.columns[3: -11].tolist()\n",
    "\n",
    "newColumns = {}\n",
    "for ci in columns:\n",
    "    if ' ' in ci:\n",
    "        newColumns[ci] = ci.split(' ',1)[0]\n",
    "    elif '(' in ci:\n",
    "        newColumns[ci] = ci.split('(',1)[0]\n",
    "    else:\n",
    "        newColumns[ci] = ci\n",
    "\n",
    "dfShortNames = df36Descriptor[columns].rename(columns=newColumns)\n",
    "\n",
    "# reduce columns to only contain MOF features\n",
    "shared_descriptor = [col for col in dfShortNames.columns if col in newColumns]\n",
    "dfMLReduced = dfShortNames[shared_descriptor]\n",
    "\n",
    "# the MOFs in \"dfMLReduced\" and adsorption data sets are different, so it is necessary to match the MOFs in two datasets\n",
    "def datasetMatch(MOFName):\n",
    "    dfML= dfMLReduced[dfMLReduced['MOF'].isin(MOFName)].drop_duplicates()\n",
    "    matchedMOFIndex=np.isin(MOFName, dfML['MOF'].values)\n",
    "    return matchedMOFIndex, dfML\n",
    "\n",
    "# read flexibility data\n",
    "flexibilityList=os.listdir('data/flexibility_data/y_data/adsorption_data') # obtain list of csv files for 9 adsorption uptakes\n",
    "flexivilityData=[]\n",
    "adsorbateNameList = []\n",
    "\n",
    "for i, name in enumerate(flexibilityList):\n",
    "    # read csv files for certain adsorption uptakes\n",
    "    df = pd.read_csv('data/flexibility_data/y_data/adsorption_data/' + name)\n",
    "    \n",
    "    # obtain the rigid value\n",
    "    rigidValue = np.array(df[df.columns[1]], dtype = float)\n",
    "    \n",
    "    # obtain the flexible mean value\n",
    "    flexValue = np.mean(np.array(df[df.columns[2:]],dtype=float),axis=1)\n",
    "    \n",
    "    # obtain the adsorbate label\n",
    "    label = np.array([name.split(\"_\")[1] for x in range(0,len(flexValue))],dtype=str)\n",
    "    adsorbateNameList.append(name.split(\"_\")[1])\n",
    "    \n",
    "    # stack the rigid value, flexible mean value and the adsorbate label\n",
    "    singleSet = np.column_stack([rigidValue,flexValue,label])\n",
    "\n",
    "    if i == 0:\n",
    "        # obtain the name list of MOFs\n",
    "        MOFNameTemp = np.array(df[df.columns[0]], dtype = str)\n",
    "        MOFName = [x.split(\"_\")[0] for x in MOFNameTemp]\n",
    "        \n",
    "        # search the MOF name in \"dfMLReduced\", generating dfML\n",
    "        matchedMOFIndex, dfML = datasetMatch(MOFName)\n",
    "        print(\"The number of MOFs shared by two datasets are: {:d}.\".format(dfML.shape[0]))\n",
    "        \n",
    "        # generating flexibilityData as \"y\"\n",
    "        flexibilityData = singleSet[matchedMOFIndex,:].copy()\n",
    "    else:\n",
    "        # concatenate \"y\"\n",
    "        flexibilityData = np.concatenate([flexibilityData.copy(),singleSet[matchedMOFIndex,:].copy()])\n",
    "\n",
    "# manually add adsorbate descriptors\n",
    "\n",
    "# Mw/gr.mol-1, Tc/K, Pc/bar, ω, Tb/K, Tf/K\n",
    "\n",
    "adsorbateData=np.array([\n",
    "    ['xenon',131.293,289.7,58.4,0.008,164.87,161.2], \n",
    "    ['butane',58.1,449.8,39.5,0.3,280.1,146.7], \n",
    "    ['propene',42.1,436.9,51.7,0.2,254.8,150.6], \n",
    "    ['ethane',30.1,381.8,50.3,0.2,184.0,126.2], \n",
    "    ['propane',44.1,416.5,44.6,0.2,230.1,136.5], \n",
    "    ['CO2',44.0,295.9,71.8,0.2,317.4,204.9], \n",
    "    ['ethene',28.054,282.5,51.2,0.089,169.3,228], \n",
    "    ['methane',16.04,190.4,46.0,0.011,111.5,91],\n",
    "    ['krypton',83.798,209.4,55.0,0.005,119.6,115.6]])\n",
    "\n",
    "adsorbateData.shape\n",
    "adDf = pd.DataFrame(data=adsorbateData, columns=[\"adsorbate\", \"Mw/gr.mol-1\", \"Tc/K\", \"Pc/bar\", \"ω\", \"Tb/K\", \"Tf/K\"])\n",
    "\n",
    "# sort the dataframe based on adsorbateNameList\n",
    "sorterIndex = dict(zip(adsorbateNameList,range(len(adsorbateNameList))))\n",
    "adDf['an_Rank'] = adDf['adsorbate'].map(sorterIndex)\n",
    "adDf.sort_values(['an_Rank'],ascending = [True], inplace = True)\n",
    "adDf.drop('an_Rank', 1, inplace = True)\n",
    "adDfFloat = adDf.iloc[:, 1:].astype(np.float)\n",
    "adDfFloat[\"adsorbate\"] = adDf[\"adsorbate\"]\n",
    "\n",
    "\n",
    "# replicate dfML for 9 adsorbates\n",
    "dfMLReplicate = pd.concat([dfML]*9)\n",
    "\n",
    "# replicate adDf for 89 MOFs\n",
    "adDfReplicate = pd.DataFrame(np.repeat(adDfFloat.values,89,axis=0))\n",
    "adDfReplicate.columns = adDfFloat.columns\n",
    "\n",
    "# concatenate two datasets\n",
    "dfMLReplicate.reset_index(drop=True, inplace=True)\n",
    "adDfReplicate.reset_index(drop=True, inplace=True)\n",
    "XAllDescriptor = pd.concat([dfMLReplicate, adDfReplicate],axis=1)\n",
    "\n",
    "X = np.concatenate((XAllDescriptor.iloc[:, 1:-1], flexibilityData[:, 0].reshape(-1, 1)),axis=1).astype(np.float)\n",
    "y = flexibilityData[:, 1].astype('float64') .reshape(-1,1)\n",
    "\n",
    "\n",
    "# feature scaling\n",
    "X_scaled = (X - X.mean(axis=0))/X.std(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data set split\n",
    "## <span style=\"color:red\">Validation set split (change)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# combine the unscaled and scaled X, so that they can be split together\n",
    "X_combined = np.concatenate((X, X_scaled), axis=1)\n",
    "\n",
    "# ---------------------------- don't touch the validation set ----------------------------\n",
    "X_train_test_combined, X_validation_combined, y_train_test, y_validation = train_test_split(X_combined, \\\n",
    "                                                                                            y, test_size=0.25)\n",
    "X_train_test, X_train_test_scaled = X_train_test_combined[:, :35], X_train_test_combined[:, 35:]\n",
    "X_validation, X_validation_scaled = X_validation_combined[:, :35], X_validation_combined[:, 35:]\n",
    "# ---------------------------- don't touch the validation set ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold on train-test set <span style=\"color:red\">(no change, but we are going to use kf defined here)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle = True, random_state=20)\n",
    "\n",
    "cv_splitter = kf.split(X_train_test_combined)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv_splitter):\n",
    "    \n",
    "    # initialize sets\n",
    "    if i == 0:\n",
    "        X_train_combined_5fold = np.zeros(X_train_test_combined[train_index].shape + (5,))\n",
    "        X_test_combined_5fold = np.zeros(X_train_test_combined[test_index].shape + (5,))\n",
    "        y_train_5fold = np.zeros(y_train_test[train_index].shape + (5,))\n",
    "        y_test_5fold = np.zeros(y_train_test[test_index].shape + (5,))\n",
    "    \n",
    "    X_train_combined_5fold[:, :, i], X_test_combined_5fold[:, :, i] = X_train_test_combined[train_index], X_train_test_combined[test_index]\n",
    "    y_train_5fold[:, :, i], y_test_5fold[:, :, i] = y_train_test[train_index], y_train_test[test_index]\n",
    "\n",
    "X_train_5fold, X_train_scaled_5fold = X_train_combined_5fold[:, :35], X_train_combined_5fold[:, 35:]\n",
    "X_test_5fold, X_test_scaled_5fold = X_test_combined_5fold[:, :35], X_test_combined_5fold[:, 35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show how to use 5-fold sets with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">data used in GridSearchCV: `X_train_test`, `y_train_test`. They are the whole dataset excluding validation set. </span>\n",
    "Notice their dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 35)\n",
      "(600, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_test.shape)\n",
    "print(y_train_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use scaled data, simply replace `X_train_test` with `X_train_test_scaled`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">cv method for GridSearchCV: `cv=kf`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "loss_fun = 'logistic'\n",
    "\n",
    "print(\"Initial search space: {}\".format(layer_list))\n",
    "layer_dict = {\n",
    "    'hidden_layer_sizes': [1] + list(range(10, 110, 10))\n",
    "}\n",
    "\n",
    "model_NN_1 = MLPRegressor(max_iter=200, activation=loss_fun)\n",
    "model_NN_1_search = GridSearchCV(model_NN_1, layer_dict, n_jobs=-1, cv=kf)\n",
    "model_NN_1_search.fit(X_train_test, y_train_test)\n",
    "\n",
    "\n",
    "\n",
    "model_NN_1_best = model_NN_1_search.best_estimator_\n",
    "print(\"Best model score: {}\".format(model_NN_1_search.best_score_))"
   ]
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "MarkdownCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "361px",
    "width": "435px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
