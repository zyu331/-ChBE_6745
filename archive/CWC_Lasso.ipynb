{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"6\"><b>Showcasing new validation strategy</b></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training strategy\n",
    "\n",
    "1. Multi-linear regression\n",
    "2. Kernel regression\n",
    "3. Kernel ridge regression\n",
    "4. Lasso regression\n",
    "5. Genetic programming\n",
    "6. Neural network\n",
    "\n",
    "We will try the multi-linear and kernel regression first as our base-line model optimized, and then follow by parameter regulation methods like Kerel Ridge and Lasso regression. Apart from the methods which had been already introduced in the lectures, we will go further to newral network method. These 3 methods are covered in our imporved model. Our goal is to determine the best hyperparameters for each model based on the $r_{square}$ values of testsets.\n",
    "\n",
    "## Model validation strategy\n",
    "\n",
    "1. hold-out\n",
    "2. k-fold\n",
    "3. bootstrapping\n",
    "\n",
    "Currently 25% of our original dataset is randomly assign as validation set which would not be modified or changed throughout the training-testing process. In our final report, we will probabibly select datapoints where there is a significant difference between the rigid/flexible models to see if the models can also predict extreme cases. We will do validations (the above 3 methods) based on the determined hyperparameters for all the training strategies. That will also be covered in our final report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unchanged codes\n",
    "Unchanged codes are lumped together below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble, add some code for formatting, not related tto validation strategy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "np.set_printoptions(precision=6,suppress=True)\n",
    "plt.rcParams['savefig.dpi'] = 75\n",
    "\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.serif'] = \"cm\"\n",
    "plt.rcParams['text.latex.preamble']= r\"\\usepackage{subdepth}, \\usepackage{type1cm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of MOFs shared by two datasets are: 89.\n"
     ]
    }
   ],
   "source": [
    "# read the 36-descriptor data\n",
    "df36Descriptor = pd.read_excel('data/ML_data/descriptor_used.xlsx',header=4,index_col=1)\n",
    "\n",
    "# clean up the column\n",
    "columns = [df36Descriptor.columns[1]] + df36Descriptor.columns[3: -11].tolist()\n",
    "\n",
    "newColumns = {}\n",
    "for ci in columns:\n",
    "    if ' ' in ci:\n",
    "        newColumns[ci] = ci.split(' ',1)[0]\n",
    "    elif '(' in ci:\n",
    "        newColumns[ci] = ci.split('(',1)[0]\n",
    "    else:\n",
    "        newColumns[ci] = ci\n",
    "\n",
    "dfShortNames = df36Descriptor[columns].rename(columns=newColumns)\n",
    "\n",
    "# reduce columns to only contain MOF features\n",
    "shared_descriptor = [col for col in dfShortNames.columns if col in newColumns]\n",
    "dfMLReduced = dfShortNames[shared_descriptor]\n",
    "\n",
    "# the MOFs in \"dfMLReduced\" and adsorption data sets are different, so it is necessary to match the MOFs in two datasets\n",
    "def datasetMatch(MOFName):\n",
    "    dfML= dfMLReduced[dfMLReduced['MOF'].isin(MOFName)].drop_duplicates()\n",
    "    matchedMOFIndex=np.isin(MOFName, dfML['MOF'].values)\n",
    "    return matchedMOFIndex, dfML\n",
    "\n",
    "# read flexibility data\n",
    "flexibilityList=os.listdir('data/flexibility_data/y_data/adsorption_data') # obtain list of csv files for 9 adsorption uptakes\n",
    "flexivilityData=[]\n",
    "adsorbateNameList = []\n",
    "\n",
    "for i, name in enumerate(flexibilityList):\n",
    "    # read csv files for certain adsorption uptakes\n",
    "    df = pd.read_csv('data/flexibility_data/y_data/adsorption_data/' + name)\n",
    "    \n",
    "    # obtain the rigid value\n",
    "    rigidValue = np.array(df[df.columns[1]], dtype = float)\n",
    "    \n",
    "    # obtain the flexible mean value\n",
    "    flexValue = np.mean(np.array(df[df.columns[2:]],dtype=float),axis=1)\n",
    "    \n",
    "    # obtain the adsorbate label\n",
    "    label = np.array([name.split(\"_\")[1] for x in range(0,len(flexValue))],dtype=str)\n",
    "    adsorbateNameList.append(name.split(\"_\")[1])\n",
    "    \n",
    "    # stack the rigid value, flexible mean value and the adsorbate label\n",
    "    singleSet = np.column_stack([rigidValue,flexValue,label])\n",
    "\n",
    "    if i == 0:\n",
    "        # obtain the name list of MOFs\n",
    "        MOFNameTemp = np.array(df[df.columns[0]], dtype = str)\n",
    "        MOFName = [x.split(\"_\")[0] for x in MOFNameTemp]\n",
    "        \n",
    "        # search the MOF name in \"dfMLReduced\", generating dfML\n",
    "        matchedMOFIndex, dfML = datasetMatch(MOFName)\n",
    "        print(\"The number of MOFs shared by two datasets are: {:d}.\".format(dfML.shape[0]))\n",
    "        \n",
    "        # generating flexibilityData as \"y\"\n",
    "        flexibilityData = singleSet[matchedMOFIndex,:].copy()\n",
    "    else:\n",
    "        # concatenate \"y\"\n",
    "        flexibilityData = np.concatenate([flexibilityData.copy(),singleSet[matchedMOFIndex,:].copy()])\n",
    "\n",
    "# manually add adsorbate descriptors\n",
    "\n",
    "# Mw/gr.mol-1, Tc/K, Pc/bar, ω, Tb/K, Tf/K\n",
    "\n",
    "adsorbateData=np.array([\n",
    "    ['xenon',131.293,289.7,58.4,0.008,164.87,161.2], \n",
    "    ['butane',58.1,449.8,39.5,0.3,280.1,146.7], \n",
    "    ['propene',42.1,436.9,51.7,0.2,254.8,150.6], \n",
    "    ['ethane',30.1,381.8,50.3,0.2,184.0,126.2], \n",
    "    ['propane',44.1,416.5,44.6,0.2,230.1,136.5], \n",
    "    ['CO2',44.0,295.9,71.8,0.2,317.4,204.9], \n",
    "    ['ethene',28.054,282.5,51.2,0.089,169.3,228], \n",
    "    ['methane',16.04,190.4,46.0,0.011,111.5,91],\n",
    "    ['krypton',83.798,209.4,55.0,0.005,119.6,115.6]])\n",
    "\n",
    "adsorbateData.shape\n",
    "adDf = pd.DataFrame(data=adsorbateData, columns=[\"adsorbate\", \"Mw/gr.mol-1\", \"Tc/K\", \"Pc/bar\", \"ω\", \"Tb/K\", \"Tf/K\"])\n",
    "\n",
    "# sort the dataframe based on adsorbateNameList\n",
    "sorterIndex = dict(zip(adsorbateNameList,range(len(adsorbateNameList))))\n",
    "adDf['an_Rank'] = adDf['adsorbate'].map(sorterIndex)\n",
    "adDf.sort_values(['an_Rank'],ascending = [True], inplace = True)\n",
    "adDf.drop('an_Rank', 1, inplace = True)\n",
    "adDfFloat = adDf.iloc[:, 1:].astype(np.float)\n",
    "adDfFloat[\"adsorbate\"] = adDf[\"adsorbate\"]\n",
    "\n",
    "\n",
    "# replicate dfML for 9 adsorbates\n",
    "dfMLReplicate = pd.concat([dfML]*9)\n",
    "\n",
    "# replicate adDf for 89 MOFs\n",
    "adDfReplicate = pd.DataFrame(np.repeat(adDfFloat.values,89,axis=0))\n",
    "adDfReplicate.columns = adDfFloat.columns\n",
    "\n",
    "# concatenate two datasets\n",
    "dfMLReplicate.reset_index(drop=True, inplace=True)\n",
    "adDfReplicate.reset_index(drop=True, inplace=True)\n",
    "XAllDescriptor = pd.concat([dfMLReplicate, adDfReplicate],axis=1)\n",
    "\n",
    "X = np.concatenate((XAllDescriptor.iloc[:, 1:-1], flexibilityData[:, 0].reshape(-1, 1)),axis=1).astype(np.float)\n",
    "y = flexibilityData[:, 1].astype('float64') .reshape(-1,1)\n",
    "\n",
    "\n",
    "# feature scaling\n",
    "X_scaled = (X - X.mean(axis=0))/X.std(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data set split\n",
    "## Validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# combine the unscaled and scaled X, so that they can be split together\n",
    "X_combined = np.concatenate((X, X_scaled), axis=1)\n",
    "\n",
    "# ---------------------------- don't touch the validation set ----------------------------\n",
    "X_train_test_combined, X_validation_combined, y_train_test, y_validation = train_test_split(X_combined, \\\n",
    "                                                                                            y, test_size=0.25)\n",
    "X_train_test, X_train_test_scaled = X_train_test_combined[:, :35], X_train_test_combined[:, 35:]\n",
    "X_validation, X_validation_scaled = X_validation_combined[:, :35], X_validation_combined[:, 35:]\n",
    "# ---------------------------- don't touch the validation set ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold on train-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_train_test_combined)):\n",
    "    \n",
    "    # initialize sets\n",
    "    if i == 0:\n",
    "        X_train_combined_5fold = np.zeros(X_train_test_combined[train_index].shape + (5,))\n",
    "        X_test_combined_5fold = np.zeros(X_train_test_combined[test_index].shape + (5,))\n",
    "        y_train_5fold = np.zeros(y_train_test[train_index].shape + (5,))\n",
    "        y_test_5fold = np.zeros(y_train_test[test_index].shape + (5,))\n",
    "        \n",
    "    X_train_combined_5fold[:, :, i], X_test_combined_5fold[:, :, i] = X_train_test_combined[train_index], X_train_test_combined[test_index]\n",
    "    y_train_5fold[:, :, i], y_test_5fold[:, :, i] = y_train_test[train_index], y_train_test[test_index]\n",
    "\n",
    "X_train_5fold, X_train_scaled_5fold = X_train_combined_5fold[:, :35], X_train_combined_5fold[:, 35:]\n",
    "X_test_5fold, X_test_scaled_5fold = X_test_combined_5fold[:, :35], X_test_combined_5fold[:, 35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show how to use 5-fold sets\n",
    "\n",
    "The code below is only meant for showing how to use 5-fold strategy for training models.\n",
    "<span style=\"color:red\">They are **not** examples of the whole modeling work</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without hyperparameter tuning - using multi-linear regression as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\tr2=0.983\n",
      "Fold 1:\tr2=0.984\t<-best performance\n",
      "Fold 2:\tr2=0.981\n",
      "Fold 3:\tr2=0.977\n",
      "Fold 4:\tr2=0.970\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LR_r2_list = []\n",
    "LR_coef_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    LR_model = LinearRegression()\n",
    "    LR_model.fit(X_train_5fold[:, :, i], y_train_5fold[:, :, i])\n",
    "    \n",
    "    LR_r2_list.append(LR_model.score(X_test_5fold[:, :, i], y_test_5fold[:, :, i]))\n",
    "    LR_coef_list.append(LR_model.coef_)\n",
    "\n",
    "\n",
    "best_i = np.array(LR_r2_list).argmax()\n",
    "LR_coef_best = LR_coef_list[best_i]\n",
    "\n",
    "for i in range(5):\n",
    "    if i != best_i:\n",
    "        print(\"Fold {}:\\tr2={:.3f}\".format(i, LR_r2_list[i]))\n",
    "    else:\n",
    "        print(\"Fold {}:\\tr2={:.3f}\\t<-best performance\".format(i, LR_r2_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With hyperparameter tuning - using rbf kernel regression as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\tr2=0.031,\toptimal gamma=2e+00\n",
      "Fold 1:\tr2=0.047,\toptimal gamma=5e-01\t\t<-best performance\n",
      "Fold 2:\tr2=0.030,\toptimal gamma=5e-01\n",
      "Fold 3:\tr2=0.028,\toptimal gamma=5e+01\n",
      "Fold 4:\tr2=0.031,\toptimal gamma=5e+01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "sigmas = np.array([1E-4, 5E-4, 1E-3, 5E-3, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 15, 20, 25, 30, 40, 50, 60])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "r2_matrix = np.zeros((gammas.size, 3))\n",
    "\n",
    "rbf_r2_list = []\n",
    "rbf_coef_list = []\n",
    "rbf_gamma_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    rbf_coef_temp_list = [] # temporarily store parameter values for each gamma\n",
    "    \n",
    "    for j, gamma in enumerate(gammas):\n",
    "    \n",
    "        X_train_kernel = rbf_kernel(X_train_5fold[:, :, i], X_train_5fold[:, :, i], gamma = gamma)\n",
    "        X_test_kernel = rbf_kernel(X_test_5fold[:, :, i], X_train_5fold[:, :, i], gamma = gamma)\n",
    "        model_rbf = LinearRegression()\n",
    "        model_rbf.fit(X_train_kernel, y_train_5fold[:, :, i])\n",
    "\n",
    "        r2_matrix[j, 0] = gamma\n",
    "        r2_matrix[j, 1] = model_rbf.score(X_train_kernel, y_train_5fold[:, :, i])\n",
    "        r2_matrix[j, -1] = model_rbf.score(X_test_kernel, y_test_5fold[:, :, i])\n",
    "        \n",
    "        rbf_coef_temp_list.append(model_rbf.coef_)\n",
    "\n",
    "    # For each fold, record the optimal gamma and the corresponding parameter values and r2\n",
    "    n = r2_matrix[:, -1].argmax()\n",
    "    rbf_r2_list.append(r2_matrix[n, -1])\n",
    "    rbf_gamma_list.append(r2_matrix[n, 0])\n",
    "    rbf_coef_list.append(rbf_coef_temp_list[n])\n",
    "\n",
    "\n",
    "best_i = np.array(rbf_r2_list).argmax()\n",
    "rbf_gamma_best = rbf_gamma_list[best_i]\n",
    "rbf_coef_best = rbf_coef_list[best_i]\n",
    "\n",
    "for i in range(5):\n",
    "    if i != best_i:\n",
    "        print(\"Fold {}:\\tr2={:.3f},\\toptimal gamma={:.0e}\".format(i, rbf_r2_list[i], rbf_gamma_list[i]))\n",
    "    else:\n",
    "        print(\"Fold {}:\\tr2={:.3f},\\toptimal gamma={:.0e}\\t\\t<-best performance\".format(i, rbf_r2_list[i], rbf_gamma_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Again, the code above does not correspond to the entire work for a model. More work can be done, e.g., \n",
    "    - detailed analysis of results\n",
    "    - more sophisticated hyperparameter tuning\n",
    "    - comparison of using unscaled and scaled data\n",
    "    - comparison with other models, especially for pure rbf, KRR and LASSO\n",
    "- To use the scaled X, replace `X_train_5fold`, `X_test_5fold` with `X_train_scaled_5fold`, `X_test_scaled_5fold`\n",
    "- Remember to record the values of the parameters and the hyperparameters of the best model, as they will be used for future comparison. For example, in \"rbf kernel regression\" above, I have recorded the optimal hyperparameter in `rbf_gamma_best` and the corresponding optimal parameters in `rbf_coef_best`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO Regression - Multi-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is a type of linear regression that uses shrinkage. Lass0 is the abreviation of **L**east **A**bsolute **S**hrinkage and **S**election **O**perator. This particular type of regression is well-suited for models showing high levels of muticollinearity or to automate certain parts of model selection, such as parameter elimination. As we have been taught during the lecture, the loss function for LASSO is defined as:\n",
    "\n",
    "$L_{LASSO} = \\sum_i \\epsilon_i^2 + \\alpha ||\\vec{w}||_1$\n",
    "\n",
    "Despite the fact that our multi-linear model works very well, overfitting is still a big issue that we have 35 descriptor but only 802 data points ($< 35^2$). Parameter regularization is therefore an effectively way to avoid such complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unscaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha is: 1.0\n",
      "r square is: 0.9548690838119207\n",
      "Number of dropped features: 28\n",
      "\n",
      "======Second GridSearchCV Refinement======\n",
      "\n",
      "Best alpha is: 0.5\n",
      "r square is: 0.9652002482239666\n",
      "Number of dropped features: 29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])\n",
    "\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "LASSO = Lasso(tol=1)\n",
    "LASSO_search = GridSearchCV(LASSO, parameter_set, cv =kf)\n",
    "LASSO_search.fit(X_train_test, y_train_test)\n",
    "                     \n",
    "\n",
    "best_alpha = LASSO_search.best_estimator_.alpha\n",
    "r_2 = LASSO_search.best_score_\n",
    "coeffs = LASSO_search.best_estimator_.coef_\n",
    "nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "num_dropped = len(nonzero)\n",
    "    \n",
    "lasso_alpha_list.append(best_alpha)\n",
    "lasso_r2_list.append(r_2)\n",
    "lasso_coef_list.append(coeffs)\n",
    "lasso_num_dropped.append(num_dropped)\n",
    "    \n",
    "print('Best alpha is: {}'.format(best_alpha))\n",
    "print('r square is: {}'.format(r_2))\n",
    "print('Number of dropped features: {}'.format(num_dropped))\n",
    "\n",
    "#Second refinement\n",
    "print('\\n====== Second GridSearchCV Refinement ======\\n')\n",
    "\n",
    "alphas = np.array([0.1, 0.5, 0.7, 0.8, 0.9, 1, 2, 3, 5, 10])\n",
    "\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "LASSO = Lasso(tol=1)\n",
    "LASSO_search = GridSearchCV(LASSO, parameter_set, cv =kf)\n",
    "LASSO_search.fit(X_train_test, y_train_test)\n",
    "                     \n",
    "\n",
    "best_alpha = LASSO_search.best_estimator_.alpha\n",
    "r_2 = LASSO_search.best_score_\n",
    "coeffs = LASSO_search.best_estimator_.coef_\n",
    "nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "num_dropped = len(nonzero)\n",
    "    \n",
    "lasso_alpha_list.append(best_alpha)\n",
    "lasso_r2_list.append(r_2)\n",
    "lasso_coef_list.append(coeffs)\n",
    "lasso_num_dropped.append(num_dropped)\n",
    "    \n",
    "print('Best alpha is: {}'.format(best_alpha))\n",
    "print('r square is: {}'.format(r_2))\n",
    "print('Number of dropped features: {}'.format(num_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha is: 1.0\n",
      "r square is: 0.8878867142051691\n",
      "Number of dropped features: 33\n",
      "\n",
      "====== Second GridSearchCV Refinement ======\n",
      "\n",
      "Best alpha is: 1.0\n",
      "r square is: 0.886752037678935\n",
      "Number of dropped features: 33\n",
      "[ 1.049597 -0.        0.       -0.       -0.       -0.       -0.\n",
      " -0.        0.       -0.        0.       -0.       -0.        0.\n",
      "  0.       -0.        0.       -0.       -0.       -0.       -0.\n",
      " -0.        0.       -0.        0.        0.       -0.       -0.\n",
      " -0.       -0.        0.       -0.        0.        0.        2.171072]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])\n",
    "\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "LASSO = Lasso(tol=1)\n",
    "LASSO_search = GridSearchCV(LASSO, parameter_set, cv =kf)\n",
    "LASSO_search.fit(X_train_test_scaled, y_train_test)\n",
    "                     \n",
    "best_alpha = LASSO_search.best_estimator_.alpha\n",
    "r_2 = LASSO_search.best_score_\n",
    "coeffs = LASSO_search.best_estimator_.coef_\n",
    "nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "num_dropped = len(nonzero)\n",
    "    \n",
    "print('Best alpha is: {}'.format(best_alpha))\n",
    "print('r square is: {}'.format(r_2))\n",
    "print('Number of dropped features: {}'.format(num_dropped))\n",
    "\n",
    "#Second refinement\n",
    "print('\\n====== Second GridSearchCV Refinement ======\\n')\n",
    "\n",
    "alphas = np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])\n",
    "\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "LASSO = Lasso(tol=1)\n",
    "LASSO_search = GridSearchCV(LASSO, parameter_set, cv =kf)\n",
    "LASSO_search.fit(X_train_test_scaled, y_train_test)\n",
    "                     \n",
    "best_alpha = LASSO_search.best_estimator_.alpha\n",
    "r_2 = LASSO_search.best_score_\n",
    "coeffs = LASSO_search.best_estimator_.coef_\n",
    "nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "num_dropped = len(nonzero)\n",
    "    \n",
    "print('Best alpha is: {}'.format(best_alpha))\n",
    "print('r square is: {}'.format(r_2))\n",
    "print('Number of dropped features: {}'.format(num_dropped))\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rescaling x matirx accross data points, surprisingly, the performance of the model became slightly worse. But generally, multi-linear Lasso regression is a good appoach to shrink and largly simplify descriptors the with high accuracy (33 features are dropped based on the result, only two features are really important). This is a very exciting result that we can only use 2 most important features to describe gas uptakes for most of the cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO Regression with RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unscaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50000000.              0.001          -0.032053       77.      ]\n",
      " [ 2000000.              0.0001         -0.009047        9.      ]\n",
      " [  500000.              0.1            -0.013883      600.      ]\n",
      " [   20000.              0.1            -0.012401      600.      ]\n",
      " [    5000.              0.001          -0.042702       77.      ]\n",
      " [     200.              0.001           0.006988       75.      ]\n",
      " [      50.              0.001           0.004527       78.      ]\n",
      " [       2.              0.001           0.019609       85.      ]\n",
      " [       0.5             0.001           0.043816       94.      ]\n",
      " [       0.02            0.01            0.173415      492.      ]\n",
      " [       0.005           0.01            0.379635      466.      ]\n",
      " [       0.002222        0.001           0.482963       92.      ]\n",
      " [       0.00125         0.001           0.355095      118.      ]\n",
      " [       0.0008          0.00001         0.313189        0.      ]\n",
      " [       0.000556        0.00001         0.241696        1.      ]\n",
      " [       0.000313        0.001           0.125312      129.      ]\n",
      " [       0.0002          0.001           0.172175      123.      ]\n",
      " [       0.000139        0.001           0.193653      122.      ]]\n",
      "Best gamma is: 2e-03\n",
      "Best alpha is: 1e-03\n",
      "r square is: 0.483\n",
      "Number of dropped features: 92.0\n",
      "\n",
      "====== Second GridSearchCV Refinement ======\n",
      "\n",
      "[[  0.02       0.003      0.321495 248.      ]\n",
      " [  0.013889   0.001      0.343787  82.      ]\n",
      " [  0.010204   0.001      0.453156  89.      ]\n",
      " [  0.007812   0.003      0.417661 274.      ]\n",
      " [  0.006173   0.001      0.480372  97.      ]\n",
      " [  0.005      0.0009     0.473352  93.      ]\n",
      " [  0.004132   0.0009     0.533091  90.      ]\n",
      " [  0.003472   0.005      0.383345 362.      ]\n",
      " [  0.002959   0.0005     0.446914  42.      ]\n",
      " [  0.002551   0.0008     0.475578  65.      ]\n",
      " [  0.002222   0.0008     0.500631  66.      ]]\n",
      "Best gamma is: 4e-03\n",
      "Best alpha is: 9e-04\n",
      "r square is: 0.533\n",
      "Number of dropped features: 90.0\n"
     ]
    }
   ],
   "source": [
    "sigmas = np.array([1E-4, 5E-4, 1E-3, 5E-3, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 15, 20, 25, 30, 40, 50, 60])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "r2_matrix = np.zeros((gammas.size, 4))\n",
    "\n",
    "alphas = np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "     \n",
    "for j, gamma in enumerate(gammas):\n",
    "    \n",
    "    X_train_kernel = rbf_kernel(X_train_test, X_train_test, gamma = gamma)\n",
    "        \n",
    "    LASSO = Lasso(tol=1)\n",
    "    LASSO_search = GridSearchCV(LASSO, parameter_set, cv =kf)\n",
    "    LASSO_search.fit(X_train_kernel, y_train_test)\n",
    "\n",
    "    r2_matrix[j, 0] = gamma\n",
    "    r2_matrix[j, 1] = LASSO_search.best_estimator_.alpha\n",
    "    r2_matrix[j, 2] = LASSO_search.best_score_\n",
    "    coeffs = LASSO_search.best_estimator_.coef_\n",
    "    nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "    num_dropped = len(nonzero)\n",
    "    r2_matrix[j, -1] = int(num_dropped)\n",
    "        \n",
    "n = r2_matrix[:, 2].argmax()\n",
    "print(r2_matrix)\n",
    "\n",
    "print('Best gamma is: {:.0e}'.format(r2_matrix[n,0]))\n",
    "print('Best alpha is: {:.0e}'.format(r2_matrix[n,1]))\n",
    "print('r square is: {:.3f}'.format(r2_matrix[n,2]))\n",
    "print('Number of dropped features: {}'.format(r2_matrix[n,-1]))\n",
    "\n",
    "\n",
    "#Second refinement\n",
    "print('\\n====== Second GridSearchCV Refinement ======\\n')\n",
    "\n",
    "sigmas = np.array([5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "r2_matrix = np.zeros((gammas.size, 4))\n",
    "\n",
    "alphas = np.array([1e-4, 5e-4, 8e-4, 9e-4, 1e-3, 2e-3, 3e-3, 5e-3, 7e-3])\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "     \n",
    "for j, gamma in enumerate(gammas):\n",
    "    \n",
    "    X_train_kernel = rbf_kernel(X_train_test, X_train_test, gamma = gamma)\n",
    "        \n",
    "    LASSO = Lasso(tol=1)\n",
    "    LASSO_search = GridSearchCV(LASSO, parameter_set, cv =kf)\n",
    "    LASSO_search.fit(X_train_kernel, y_train_test)\n",
    "\n",
    "    r2_matrix[j, 0] = gamma\n",
    "    r2_matrix[j, 1] = LASSO_search.best_estimator_.alpha\n",
    "    r2_matrix[j, 2] = LASSO_search.best_score_\n",
    "    coeffs = LASSO_search.best_estimator_.coef_\n",
    "    nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "    num_dropped = len(nonzero)\n",
    "    r2_matrix[j, -1] = int(num_dropped)\n",
    "        \n",
    "n = r2_matrix[:, 2].argmax()\n",
    "print(r2_matrix)\n",
    "\n",
    "print('Best gamma is: {:.0e}'.format(r2_matrix[n,0]))\n",
    "print('Best alpha is: {:.0e}'.format(r2_matrix[n,1]))\n",
    "print('r square is: {:.3f}'.format(r2_matrix[n,2]))\n",
    "print('Number of dropped features: {}'.format(r2_matrix[n,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, unscaled X results in poor r square performances. We want to address here that using rbf kernel matrix seems to be unclever because it averages out each row into a single value (i.e. assuming all features have same contributions). Apparently, ignoring physical meanings of descriptors causes huge problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50000000.              0.0001         -0.008938        9.      ]\n",
      " [ 2000000.              0.1            -0.009016      600.      ]\n",
      " [  500000.              0.1            -0.006237      600.      ]\n",
      " [   20000.              0.0001         -0.001659        9.      ]\n",
      " [    5000.              0.1            -0.001191      600.      ]\n",
      " [     200.              0.001           0.016352       77.      ]\n",
      " [      50.              0.001           0.021307       85.      ]\n",
      " [       2.              0.01            0.069786      528.      ]\n",
      " [       0.5             0.01            0.242872      494.      ]\n",
      " [       0.02            0.00001         0.938302        0.      ]\n",
      " [       0.005           0.00001         0.923838        0.      ]\n",
      " [       0.002222        0.00001         0.915452        2.      ]\n",
      " [       0.00125         0.00001         0.909203        1.      ]\n",
      " [       0.0008          0.00001         0.911587        2.      ]\n",
      " [       0.000556        0.00001         0.908838        8.      ]\n",
      " [       0.000313        0.00001         0.909161       11.      ]\n",
      " [       0.0002          0.00001         0.905278       17.      ]\n",
      " [       0.000139        0.00001         0.906668       24.      ]]\n",
      "Best gamma is: 2e-02\n",
      "Best alpha is: 1e-05\n",
      "r square is: 0.938\n",
      "Number of dropped features: 0.0\n",
      "\n",
      "====== Second GridSearchCV Refinement ======\n",
      "\n",
      "[[   0.5         0.00005  -359.912926    6.      ]\n",
      " [   0.125       0.00005     0.348016    4.      ]\n",
      " [   0.055556    0.00005     0.89376     1.      ]\n",
      " [   0.03125     0.00005     0.935422    9.      ]\n",
      " [   0.02        0.00001     0.939911    0.      ]\n",
      " [   0.013889    0.00001     0.93973     1.      ]\n",
      " [   0.010204    0.00001     0.937097    0.      ]\n",
      " [   0.007812    0.00001     0.930294    0.      ]\n",
      " [   0.006173    0.00001     0.922564    2.      ]\n",
      " [   0.005       0.00001     0.915923    0.      ]]\n",
      "Best gamma is: 2e-02\n",
      "Best alpha is: 1e-05\n",
      "r square is: 0.940\n",
      "Number of dropped features: 0.0\n"
     ]
    }
   ],
   "source": [
    "sigmas = np.array([1E-4, 5E-4, 1E-3, 5E-3, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 15, 20, 25, 30, 40, 50, 60])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "r2_matrix = np.zeros((gammas.size, 4))\n",
    "\n",
    "alphas = np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "     \n",
    "for j, gamma in enumerate(gammas):\n",
    "    \n",
    "    X_train_kernel = rbf_kernel(X_train_test_scaled, X_train_test_scaled, gamma = gamma)\n",
    "        \n",
    "    LASSO = Lasso(tol=1)\n",
    "    LASSO_search = GridSearchCV(LASSO, parameter_set, cv =kf)\n",
    "    LASSO_search.fit(X_train_kernel, y_train_test)\n",
    "\n",
    "    r2_matrix[j, 0] = gamma\n",
    "    r2_matrix[j, 1] = LASSO_search.best_estimator_.alpha\n",
    "    r2_matrix[j, 2] = LASSO_search.best_score_\n",
    "    coeffs = LASSO_search.best_estimator_.coef_\n",
    "    nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "    num_dropped = len(nonzero)\n",
    "    r2_matrix[j, -1] = num_dropped\n",
    "        \n",
    "n = r2_matrix[:, 2].argmax()\n",
    "print(r2_matrix)\n",
    "\n",
    "print('Best gamma is: {:.0e}'.format(r2_matrix[n,0]))\n",
    "print('Best alpha is: {:.0e}'.format(r2_matrix[n,1]))\n",
    "print('r square is: {:.3f}'.format(r2_matrix[n,2]))\n",
    "print('Number of dropped features: {}'.format(r2_matrix[n,-1]))\n",
    "\n",
    "\n",
    "#Second refinement\n",
    "print('\\n====== Second GridSearchCV Refinement ======\\n')\n",
    "\n",
    "sigmas = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "r2_matrix = np.zeros((gammas.size, 4))\n",
    "\n",
    "alphas = np.array([1e-5, 2e-5, 3e-5, 4e-5, 5e-5])\n",
    "parameter_set = {'alpha':alphas}\n",
    "\n",
    "     \n",
    "for j, gamma in enumerate(gammas):\n",
    "    \n",
    "    X_train_kernel = rbf_kernel(X_train_test_scaled, X_train_test_scaled, gamma = gamma)\n",
    "        \n",
    "    LASSO = Lasso(tol=1)\n",
    "    LASSO_search = GridSearchCV(LASSO, parameter_set, cv =kf)\n",
    "    LASSO_search.fit(X_train_kernel, y_train_test)\n",
    "\n",
    "    r2_matrix[j, 0] = gamma\n",
    "    r2_matrix[j, 1] = LASSO_search.best_estimator_.alpha\n",
    "    r2_matrix[j, 2] = LASSO_search.best_score_\n",
    "    coeffs = LASSO_search.best_estimator_.coef_\n",
    "    nonzero = [f for f in np.isclose(coeffs,0) if f == True]\n",
    "    num_dropped = len(nonzero)\n",
    "    r2_matrix[j, -1] = num_dropped\n",
    "        \n",
    "n = r2_matrix[:, 2].argmax()\n",
    "print(r2_matrix)\n",
    "\n",
    "print('Best gamma is: {:.0e}'.format(r2_matrix[n,0]))\n",
    "print('Best alpha is: {:.0e}'.format(r2_matrix[n,1]))\n",
    "print('r square is: {:.3f}'.format(r2_matrix[n,2]))\n",
    "print('Number of dropped features: {}'.format(r2_matrix[n,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rescaling x matirx accross data points, there is a huge improvement of model performance. However, non of the 35x35 features is dropped, and optimal alpha values all very small for every cross-validation datasets, i.e. the model is not simplified. Here is one thing that we can do for our final report: Considering the trade-off between precision and model simplification. The goal of this work is to predict the gas uptate by feature of MOFs and adsorbates with very high accuracy. But from experimental aspect, it is definitely much more convenient if a relatively small amount of descriptors could sufficiently predict gas uptakes since features could be difficult and time-consuming to be obtained experimentally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future work, I think **forward selection** is a good way to see if the dropped features of two methods are similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "MarkdownCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "361px",
    "width": "435px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
